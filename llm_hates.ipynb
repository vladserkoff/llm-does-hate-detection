{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b932c9cf-423d-4354-ada2-9035d71defed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/conda/envs/hate-detection/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "from train import main\n",
    "from helpers.data import load_dataset\n",
    "from helpers.models import generate, load_model, evaluate_model, generate\n",
    "from helpers.util import ensure_reproducibility\n",
    "from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8285e7cc-d00b-4fb0-b07d-fd4ac4f916ae",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55bb60a6-5f7b-436d-9c3c-994b4a0a4ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = 14400\n",
    "max_seq_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "130f5656-c46e-4b16-8798-a95a5fce81a2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-28 17:38:37 - train:49 - INFO - Using device cuda\n",
      "2024-01-28 17:38:37 - train:50 - INFO - Loading dataset with 14400 examples\n",
      "Map (num_proc=24): 100%|████████████████████████████████████████████████| 14400/14400 [00:01<00:00, 11308.74 examples/s]\n",
      "2024-01-28 17:38:41 - hate-detection.data:46 - INFO - Loaded dataset ucberkeley-dlab/measuring-hate-speech with 11520 training examples and 2880 test examples\n",
      "2024-01-28 17:38:41 - train:52 - INFO - Loading model unsloth/mistral-7b-instruct-v0.2-bnb-4bit\n",
      "==((====))==  Unsloth: Fast Mistral patching release 2024.1\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3090. Max memory: 23.669 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.1.2. CUDA = 8.6. CUDA Toolkit = 11.8.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.23.post1. FA = True.\n",
      " \"-____-\"     Apache 2 free license: http://github.com/unslothai/unsloth\n",
      "You passed `quantization_config` to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` attribute will be overwritten with the one you passed to `from_pretrained`.\n",
      "Unsloth 2024.1 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n",
      "2024-01-28 17:38:46 - train:60 - INFO - Fine-tuning model unsloth/mistral-7b-instruct-v0.2-bnb-4bit\n",
      "Map: 100%|██████████████████████████████████████████████████████████████| 11520/11520 [00:00<00:00, 12341.98 examples/s]\n",
      "/conda/envs/hate-detection/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:290: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "Evaluating: 100%|███████████████████████████████████████████████████████████████████████| 45/45 [03:06<00:00,  4.13s/it]\n",
      "2024-01-28 17:41:53 - train:108 - INFO - Pre-training evaluation:               precision    recall  f1-score   support\n",
      "\n",
      "        Hate       0.50      0.78      0.61      1049\n",
      "     Neutral       0.20      0.07      0.10       712\n",
      "  Supportive       0.83      0.22      0.35      1119\n",
      "     Unknown       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.39      2880\n",
      "   macro avg       0.38      0.27      0.26      2880\n",
      "weighted avg       0.55      0.39      0.38      2880\n",
      "\n",
      "2024-01-28 17:41:53 - wandb.jupyter:224 - ERROR - Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m...\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/projects/hate-detection/wandb/run-20240128_174154-d0lk05dv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/.../huggingface/runs/d0lk05dv' target=\"_blank\">unsloth/mistral-7b-instruct-v0.2-bnb-4bit</a></strong> to <a href='https://wandb.ai/.../huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/.../huggingface' target=\"_blank\">https://wandb.ai/.../huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/.../huggingface/runs/d0lk05dv' target=\"_blank\">https://wandb.ai/.../huggingface/runs/d0lk05dv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='180' max='180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [180/180 24:37, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.363100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|███████████████████████████████████████████████████████████████████████| 45/45 [19:07<00:00, 25.51s/it]\n",
      "2024-01-28 18:25:50 - train:113 - INFO - Post-training evaluation:               precision    recall  f1-score   support\n",
      "\n",
      "        Hate       0.74      0.91      0.82      1049\n",
      "     Neutral       0.66      0.31      0.42       712\n",
      "  Supportive       0.80      0.90      0.85      1119\n",
      "     Unknown       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.76      2880\n",
      "   macro avg       0.55      0.53      0.52      2880\n",
      "weighted avg       0.74      0.76      0.73      2880\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make: Entering directory '/projects/hate-detection/llama.cpp'\n",
      "I llama.cpp build info: \n",
      "I UNAME_S:   Linux\n",
      "I UNAME_P:   x86_64\n",
      "I UNAME_M:   x86_64\n",
      "I CFLAGS:    -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion \n",
      "I CXXFLAGS:  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi\n",
      "I NVCCFLAGS:  \n",
      "I LDFLAGS:    \n",
      "I CC:        cc (Ubuntu 13.2.0-4ubuntu3) 13.2.0\n",
      "I CXX:       g++ (Ubuntu 13.2.0-4ubuntu3) 13.2.0\n",
      "\n",
      "rm -vrf *.o tests/*.o *.so *.dll benchmark-matmult common/build-info.cpp *.dot *.gcno tests/*.gcno *.gcda tests/*.gcda *.gcov tests/*.gcov lcov-report gcovr-report main quantize quantize-stats perplexity imatrix embedding vdot q8dot train-text-from-scratch convert-llama2c-to-ggml simple batched batched-bench save-load-state server gguf llama-bench libllava.a llava-cli baby-llama beam-search speculative infill tokenize benchmark-matmult parallel finetune export-lora lookahead lookup passkey tests/test-c.o tests/test-llama-grammar tests/test-grammar-parser tests/test-double-float tests/test-grad0 tests/test-opt tests/test-quantize-fns tests/test-quantize-perf tests/test-sampling tests/test-tokenizer-0-llama tests/test-tokenizer-0-falcon tests/test-tokenizer-1-llama tests/test-tokenizer-1-bpe tests/test-rope tests/test-backend-ops tests/test-model-load-cancel tests/test-autorelease\n",
      "make: Leaving directory '/projects/hate-detection/llama.cpp'\n",
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 16.0 out of 30.49 RAM for saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 32/32 [00:00<00:00, 57.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Unsloth: Saving model... This might take 5 minutes for Llama-7b...\n",
      "Done.\n",
      "==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n",
      "   \\\\   /|    [0] Installing llama.cpp will take 3 minutes.\n",
      "O^O/ \\_/ \\    [1] Converting HF to GUUF 16bits will take 3 minutes.\n",
      "\\        /    [2] Converting GGUF 16bits to q8_0 will take 20 minutes.\n",
      " \"-____-\"     In total, you will have to wait around 26 minutes.\n",
      "\n",
      "Unsloth: [0] Installing llama.cpp. This will take 3 minutes...\n",
      "Unsloth: [1] Converting HF into GGUF format. This will take 3 minutes...\n",
      "Loading model file ../results/ggufed/unsloth/mistral-7b-instruct-v0.2-bnb-4bit/model-00001-of-00003.safetensors\n",
      "Loading model file ../results/ggufed/unsloth/mistral-7b-instruct-v0.2-bnb-4bit/model-00001-of-00003.safetensors\n",
      "Loading model file ../results/ggufed/unsloth/mistral-7b-instruct-v0.2-bnb-4bit/model-00002-of-00003.safetensors\n",
      "Loading model file ../results/ggufed/unsloth/mistral-7b-instruct-v0.2-bnb-4bit/model-00003-of-00003.safetensors\n",
      "params = Params(n_vocab=32000, n_embd=4096, n_layer=32, n_ctx=32768, n_ff=14336, n_head=32, n_head_kv=8, n_experts=None, n_experts_used=None, f_norm_eps=1e-05, rope_scaling_type=None, f_rope_freq_base=1000000.0, f_rope_scale=None, n_orig_ctx=None, rope_finetuned=None, ftype=<GGMLFileType.MostlyQ8_0: 7>, path_model=PosixPath('../results/ggufed/unsloth/mistral-7b-instruct-v0.2-bnb-4bit'))\n",
      "Found vocab files: {'tokenizer.model': PosixPath('../results/ggufed/unsloth/mistral-7b-instruct-v0.2-bnb-4bit/tokenizer.model'), 'vocab.json': None, 'tokenizer.json': PosixPath('../results/ggufed/unsloth/mistral-7b-instruct-v0.2-bnb-4bit/tokenizer.json')}\n",
      "Loading vocab file '../results/ggufed/unsloth/mistral-7b-instruct-v0.2-bnb-4bit/tokenizer.model', type 'spm'\n",
      "Vocab info: <SentencePieceVocab with 32000 base tokens and 0 added tokens>\n",
      "Special vocab info: <SpecialVocab with 0 merges, special tokens {'bos': 1, 'eos': 2, 'unk': 0, 'pad': 0}, add special tokens {'bos': True, 'eos': False}>\n",
      "Permuting layer 0\n",
      "Permuting layer 1\n",
      "Permuting layer 2\n",
      "Permuting layer 3\n",
      "Permuting layer 4\n",
      "Permuting layer 5\n",
      "Permuting layer 6\n",
      "Permuting layer 7\n",
      "Permuting layer 8\n",
      "Permuting layer 9\n",
      "Permuting layer 10\n",
      "Permuting layer 11\n",
      "Permuting layer 12\n",
      "Permuting layer 13\n",
      "Permuting layer 14\n",
      "Permuting layer 15\n",
      "Permuting layer 16\n",
      "Permuting layer 17\n",
      "Permuting layer 18\n",
      "Permuting layer 19\n",
      "Permuting layer 20\n",
      "Permuting layer 21\n",
      "Permuting layer 22\n",
      "Permuting layer 23\n",
      "Permuting layer 24\n",
      "Permuting layer 25\n",
      "Permuting layer 26\n",
      "Permuting layer 27\n",
      "Permuting layer 28\n",
      "Permuting layer 29\n",
      "Permuting layer 30\n",
      "Permuting layer 31\n",
      "model.embed_tokens.weight                        -> token_embd.weight                        | BF16   | [32000, 4096]\n",
      "model.layers.0.input_layernorm.weight            -> blk.0.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.0.mlp.down_proj.weight              -> blk.0.ffn_down.weight                    | BF16   | [4096, 14336]\n",
      "model.layers.0.mlp.gate_proj.weight              -> blk.0.ffn_gate.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.0.mlp.up_proj.weight                -> blk.0.ffn_up.weight                      | BF16   | [14336, 4096]\n",
      "model.layers.0.post_attention_layernorm.weight   -> blk.0.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.0.self_attn.k_proj.weight           -> blk.0.attn_k.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.0.self_attn.o_proj.weight           -> blk.0.attn_output.weight                 | BF16   | [4096, 4096]\n",
      "model.layers.0.self_attn.q_proj.weight           -> blk.0.attn_q.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.0.self_attn.v_proj.weight           -> blk.0.attn_v.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.1.input_layernorm.weight            -> blk.1.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.1.mlp.down_proj.weight              -> blk.1.ffn_down.weight                    | BF16   | [4096, 14336]\n",
      "model.layers.1.mlp.gate_proj.weight              -> blk.1.ffn_gate.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.1.mlp.up_proj.weight                -> blk.1.ffn_up.weight                      | BF16   | [14336, 4096]\n",
      "model.layers.1.post_attention_layernorm.weight   -> blk.1.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.1.self_attn.k_proj.weight           -> blk.1.attn_k.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.1.self_attn.o_proj.weight           -> blk.1.attn_output.weight                 | BF16   | [4096, 4096]\n",
      "model.layers.1.self_attn.q_proj.weight           -> blk.1.attn_q.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.1.self_attn.v_proj.weight           -> blk.1.attn_v.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.10.mlp.gate_proj.weight             -> blk.10.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.10.mlp.up_proj.weight               -> blk.10.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.10.self_attn.k_proj.weight          -> blk.10.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.10.self_attn.o_proj.weight          -> blk.10.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.10.self_attn.q_proj.weight          -> blk.10.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.10.self_attn.v_proj.weight          -> blk.10.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.2.input_layernorm.weight            -> blk.2.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.2.mlp.down_proj.weight              -> blk.2.ffn_down.weight                    | BF16   | [4096, 14336]\n",
      "model.layers.2.mlp.gate_proj.weight              -> blk.2.ffn_gate.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.2.mlp.up_proj.weight                -> blk.2.ffn_up.weight                      | BF16   | [14336, 4096]\n",
      "model.layers.2.post_attention_layernorm.weight   -> blk.2.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.2.self_attn.k_proj.weight           -> blk.2.attn_k.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.2.self_attn.o_proj.weight           -> blk.2.attn_output.weight                 | BF16   | [4096, 4096]\n",
      "model.layers.2.self_attn.q_proj.weight           -> blk.2.attn_q.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.2.self_attn.v_proj.weight           -> blk.2.attn_v.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.3.input_layernorm.weight            -> blk.3.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.3.mlp.down_proj.weight              -> blk.3.ffn_down.weight                    | BF16   | [4096, 14336]\n",
      "model.layers.3.mlp.gate_proj.weight              -> blk.3.ffn_gate.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.3.mlp.up_proj.weight                -> blk.3.ffn_up.weight                      | BF16   | [14336, 4096]\n",
      "model.layers.3.post_attention_layernorm.weight   -> blk.3.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.3.self_attn.k_proj.weight           -> blk.3.attn_k.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.3.self_attn.o_proj.weight           -> blk.3.attn_output.weight                 | BF16   | [4096, 4096]\n",
      "model.layers.3.self_attn.q_proj.weight           -> blk.3.attn_q.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.3.self_attn.v_proj.weight           -> blk.3.attn_v.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.4.input_layernorm.weight            -> blk.4.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.4.mlp.down_proj.weight              -> blk.4.ffn_down.weight                    | BF16   | [4096, 14336]\n",
      "model.layers.4.mlp.gate_proj.weight              -> blk.4.ffn_gate.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.4.mlp.up_proj.weight                -> blk.4.ffn_up.weight                      | BF16   | [14336, 4096]\n",
      "model.layers.4.post_attention_layernorm.weight   -> blk.4.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.4.self_attn.k_proj.weight           -> blk.4.attn_k.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.4.self_attn.o_proj.weight           -> blk.4.attn_output.weight                 | BF16   | [4096, 4096]\n",
      "model.layers.4.self_attn.q_proj.weight           -> blk.4.attn_q.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.4.self_attn.v_proj.weight           -> blk.4.attn_v.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.5.input_layernorm.weight            -> blk.5.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.5.mlp.down_proj.weight              -> blk.5.ffn_down.weight                    | BF16   | [4096, 14336]\n",
      "model.layers.5.mlp.gate_proj.weight              -> blk.5.ffn_gate.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.5.mlp.up_proj.weight                -> blk.5.ffn_up.weight                      | BF16   | [14336, 4096]\n",
      "model.layers.5.post_attention_layernorm.weight   -> blk.5.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.5.self_attn.k_proj.weight           -> blk.5.attn_k.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.5.self_attn.o_proj.weight           -> blk.5.attn_output.weight                 | BF16   | [4096, 4096]\n",
      "model.layers.5.self_attn.q_proj.weight           -> blk.5.attn_q.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.5.self_attn.v_proj.weight           -> blk.5.attn_v.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.6.input_layernorm.weight            -> blk.6.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.6.mlp.down_proj.weight              -> blk.6.ffn_down.weight                    | BF16   | [4096, 14336]\n",
      "model.layers.6.mlp.gate_proj.weight              -> blk.6.ffn_gate.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.6.mlp.up_proj.weight                -> blk.6.ffn_up.weight                      | BF16   | [14336, 4096]\n",
      "model.layers.6.post_attention_layernorm.weight   -> blk.6.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.6.self_attn.k_proj.weight           -> blk.6.attn_k.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.6.self_attn.o_proj.weight           -> blk.6.attn_output.weight                 | BF16   | [4096, 4096]\n",
      "model.layers.6.self_attn.q_proj.weight           -> blk.6.attn_q.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.6.self_attn.v_proj.weight           -> blk.6.attn_v.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.7.input_layernorm.weight            -> blk.7.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.7.mlp.down_proj.weight              -> blk.7.ffn_down.weight                    | BF16   | [4096, 14336]\n",
      "model.layers.7.mlp.gate_proj.weight              -> blk.7.ffn_gate.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.7.mlp.up_proj.weight                -> blk.7.ffn_up.weight                      | BF16   | [14336, 4096]\n",
      "model.layers.7.post_attention_layernorm.weight   -> blk.7.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.7.self_attn.k_proj.weight           -> blk.7.attn_k.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.7.self_attn.o_proj.weight           -> blk.7.attn_output.weight                 | BF16   | [4096, 4096]\n",
      "model.layers.7.self_attn.q_proj.weight           -> blk.7.attn_q.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.7.self_attn.v_proj.weight           -> blk.7.attn_v.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.8.input_layernorm.weight            -> blk.8.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.8.mlp.down_proj.weight              -> blk.8.ffn_down.weight                    | BF16   | [4096, 14336]\n",
      "model.layers.8.mlp.gate_proj.weight              -> blk.8.ffn_gate.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.8.mlp.up_proj.weight                -> blk.8.ffn_up.weight                      | BF16   | [14336, 4096]\n",
      "model.layers.8.post_attention_layernorm.weight   -> blk.8.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.8.self_attn.k_proj.weight           -> blk.8.attn_k.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.8.self_attn.o_proj.weight           -> blk.8.attn_output.weight                 | BF16   | [4096, 4096]\n",
      "model.layers.8.self_attn.q_proj.weight           -> blk.8.attn_q.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.8.self_attn.v_proj.weight           -> blk.8.attn_v.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.9.input_layernorm.weight            -> blk.9.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.9.mlp.down_proj.weight              -> blk.9.ffn_down.weight                    | BF16   | [4096, 14336]\n",
      "model.layers.9.mlp.gate_proj.weight              -> blk.9.ffn_gate.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.9.mlp.up_proj.weight                -> blk.9.ffn_up.weight                      | BF16   | [14336, 4096]\n",
      "model.layers.9.post_attention_layernorm.weight   -> blk.9.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.9.self_attn.k_proj.weight           -> blk.9.attn_k.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.9.self_attn.o_proj.weight           -> blk.9.attn_output.weight                 | BF16   | [4096, 4096]\n",
      "model.layers.9.self_attn.q_proj.weight           -> blk.9.attn_q.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.9.self_attn.v_proj.weight           -> blk.9.attn_v.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.10.input_layernorm.weight           -> blk.10.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.10.mlp.down_proj.weight             -> blk.10.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.10.post_attention_layernorm.weight  -> blk.10.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.11.input_layernorm.weight           -> blk.11.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.11.mlp.down_proj.weight             -> blk.11.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.11.mlp.gate_proj.weight             -> blk.11.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.11.mlp.up_proj.weight               -> blk.11.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.11.post_attention_layernorm.weight  -> blk.11.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.11.self_attn.k_proj.weight          -> blk.11.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.11.self_attn.o_proj.weight          -> blk.11.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.11.self_attn.q_proj.weight          -> blk.11.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.11.self_attn.v_proj.weight          -> blk.11.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.12.input_layernorm.weight           -> blk.12.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.12.mlp.down_proj.weight             -> blk.12.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.12.mlp.gate_proj.weight             -> blk.12.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.12.mlp.up_proj.weight               -> blk.12.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.12.post_attention_layernorm.weight  -> blk.12.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.12.self_attn.k_proj.weight          -> blk.12.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.12.self_attn.o_proj.weight          -> blk.12.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.12.self_attn.q_proj.weight          -> blk.12.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.12.self_attn.v_proj.weight          -> blk.12.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.13.input_layernorm.weight           -> blk.13.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.13.mlp.down_proj.weight             -> blk.13.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.13.mlp.gate_proj.weight             -> blk.13.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.13.mlp.up_proj.weight               -> blk.13.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.13.post_attention_layernorm.weight  -> blk.13.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.13.self_attn.k_proj.weight          -> blk.13.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.13.self_attn.o_proj.weight          -> blk.13.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.13.self_attn.q_proj.weight          -> blk.13.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.13.self_attn.v_proj.weight          -> blk.13.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.14.input_layernorm.weight           -> blk.14.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.14.mlp.down_proj.weight             -> blk.14.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.14.mlp.gate_proj.weight             -> blk.14.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.14.mlp.up_proj.weight               -> blk.14.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.14.post_attention_layernorm.weight  -> blk.14.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.14.self_attn.k_proj.weight          -> blk.14.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.14.self_attn.o_proj.weight          -> blk.14.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.14.self_attn.q_proj.weight          -> blk.14.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.14.self_attn.v_proj.weight          -> blk.14.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.15.input_layernorm.weight           -> blk.15.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.15.mlp.down_proj.weight             -> blk.15.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.15.mlp.gate_proj.weight             -> blk.15.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.15.mlp.up_proj.weight               -> blk.15.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.15.post_attention_layernorm.weight  -> blk.15.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.15.self_attn.k_proj.weight          -> blk.15.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.15.self_attn.o_proj.weight          -> blk.15.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.15.self_attn.q_proj.weight          -> blk.15.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.15.self_attn.v_proj.weight          -> blk.15.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.16.input_layernorm.weight           -> blk.16.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.16.mlp.down_proj.weight             -> blk.16.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.16.mlp.gate_proj.weight             -> blk.16.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.16.mlp.up_proj.weight               -> blk.16.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.16.post_attention_layernorm.weight  -> blk.16.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.16.self_attn.k_proj.weight          -> blk.16.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.16.self_attn.o_proj.weight          -> blk.16.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.16.self_attn.q_proj.weight          -> blk.16.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.16.self_attn.v_proj.weight          -> blk.16.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.17.input_layernorm.weight           -> blk.17.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.17.mlp.down_proj.weight             -> blk.17.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.17.mlp.gate_proj.weight             -> blk.17.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.17.mlp.up_proj.weight               -> blk.17.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.17.post_attention_layernorm.weight  -> blk.17.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.17.self_attn.k_proj.weight          -> blk.17.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.17.self_attn.o_proj.weight          -> blk.17.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.17.self_attn.q_proj.weight          -> blk.17.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.17.self_attn.v_proj.weight          -> blk.17.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.18.input_layernorm.weight           -> blk.18.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.18.mlp.down_proj.weight             -> blk.18.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.18.mlp.gate_proj.weight             -> blk.18.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.18.mlp.up_proj.weight               -> blk.18.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.18.post_attention_layernorm.weight  -> blk.18.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.18.self_attn.k_proj.weight          -> blk.18.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.18.self_attn.o_proj.weight          -> blk.18.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.18.self_attn.q_proj.weight          -> blk.18.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.18.self_attn.v_proj.weight          -> blk.18.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.19.input_layernorm.weight           -> blk.19.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.19.mlp.down_proj.weight             -> blk.19.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.19.mlp.gate_proj.weight             -> blk.19.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.19.mlp.up_proj.weight               -> blk.19.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.19.post_attention_layernorm.weight  -> blk.19.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.19.self_attn.k_proj.weight          -> blk.19.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.19.self_attn.o_proj.weight          -> blk.19.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.19.self_attn.q_proj.weight          -> blk.19.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.19.self_attn.v_proj.weight          -> blk.19.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.20.input_layernorm.weight           -> blk.20.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.20.mlp.down_proj.weight             -> blk.20.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.20.mlp.gate_proj.weight             -> blk.20.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.20.mlp.up_proj.weight               -> blk.20.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.20.post_attention_layernorm.weight  -> blk.20.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.20.self_attn.k_proj.weight          -> blk.20.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.20.self_attn.o_proj.weight          -> blk.20.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.20.self_attn.q_proj.weight          -> blk.20.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.20.self_attn.v_proj.weight          -> blk.20.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.21.input_layernorm.weight           -> blk.21.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.21.mlp.down_proj.weight             -> blk.21.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.21.mlp.gate_proj.weight             -> blk.21.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.21.mlp.up_proj.weight               -> blk.21.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.21.post_attention_layernorm.weight  -> blk.21.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.21.self_attn.k_proj.weight          -> blk.21.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.21.self_attn.o_proj.weight          -> blk.21.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.21.self_attn.q_proj.weight          -> blk.21.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.21.self_attn.v_proj.weight          -> blk.21.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.22.self_attn.k_proj.weight          -> blk.22.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.22.self_attn.o_proj.weight          -> blk.22.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.22.self_attn.q_proj.weight          -> blk.22.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.22.self_attn.v_proj.weight          -> blk.22.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "lm_head.weight                                   -> output.weight                            | BF16   | [32000, 4096]\n",
      "model.layers.22.input_layernorm.weight           -> blk.22.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.22.mlp.down_proj.weight             -> blk.22.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.22.mlp.gate_proj.weight             -> blk.22.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.22.mlp.up_proj.weight               -> blk.22.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.22.post_attention_layernorm.weight  -> blk.22.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.23.input_layernorm.weight           -> blk.23.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.23.mlp.down_proj.weight             -> blk.23.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.23.mlp.gate_proj.weight             -> blk.23.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.23.mlp.up_proj.weight               -> blk.23.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.23.post_attention_layernorm.weight  -> blk.23.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.23.self_attn.k_proj.weight          -> blk.23.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.23.self_attn.o_proj.weight          -> blk.23.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.23.self_attn.q_proj.weight          -> blk.23.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.23.self_attn.v_proj.weight          -> blk.23.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.24.input_layernorm.weight           -> blk.24.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.24.mlp.down_proj.weight             -> blk.24.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.24.mlp.gate_proj.weight             -> blk.24.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.24.mlp.up_proj.weight               -> blk.24.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.24.post_attention_layernorm.weight  -> blk.24.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.24.self_attn.k_proj.weight          -> blk.24.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.24.self_attn.o_proj.weight          -> blk.24.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.24.self_attn.q_proj.weight          -> blk.24.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.24.self_attn.v_proj.weight          -> blk.24.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.25.input_layernorm.weight           -> blk.25.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.25.mlp.down_proj.weight             -> blk.25.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.25.mlp.gate_proj.weight             -> blk.25.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.25.mlp.up_proj.weight               -> blk.25.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.25.post_attention_layernorm.weight  -> blk.25.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.25.self_attn.k_proj.weight          -> blk.25.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.25.self_attn.o_proj.weight          -> blk.25.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.25.self_attn.q_proj.weight          -> blk.25.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.25.self_attn.v_proj.weight          -> blk.25.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.26.input_layernorm.weight           -> blk.26.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.26.mlp.down_proj.weight             -> blk.26.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.26.mlp.gate_proj.weight             -> blk.26.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.26.mlp.up_proj.weight               -> blk.26.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.26.post_attention_layernorm.weight  -> blk.26.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.26.self_attn.k_proj.weight          -> blk.26.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.26.self_attn.o_proj.weight          -> blk.26.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.26.self_attn.q_proj.weight          -> blk.26.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.26.self_attn.v_proj.weight          -> blk.26.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.27.input_layernorm.weight           -> blk.27.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.27.mlp.down_proj.weight             -> blk.27.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.27.mlp.gate_proj.weight             -> blk.27.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.27.mlp.up_proj.weight               -> blk.27.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.27.post_attention_layernorm.weight  -> blk.27.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.27.self_attn.k_proj.weight          -> blk.27.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.27.self_attn.o_proj.weight          -> blk.27.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.27.self_attn.q_proj.weight          -> blk.27.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.27.self_attn.v_proj.weight          -> blk.27.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.28.input_layernorm.weight           -> blk.28.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.28.mlp.down_proj.weight             -> blk.28.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.28.mlp.gate_proj.weight             -> blk.28.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.28.mlp.up_proj.weight               -> blk.28.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.28.post_attention_layernorm.weight  -> blk.28.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.28.self_attn.k_proj.weight          -> blk.28.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.28.self_attn.o_proj.weight          -> blk.28.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.28.self_attn.q_proj.weight          -> blk.28.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.28.self_attn.v_proj.weight          -> blk.28.attn_v.weight                     | BF16   | [1024, 4096]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/conda/envs/hate-detection/lib/python3.10/subprocess.py:961: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdout = io.open(c2pread, 'rb', bufsize)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.29.input_layernorm.weight           -> blk.29.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.29.mlp.down_proj.weight             -> blk.29.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.29.mlp.gate_proj.weight             -> blk.29.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.29.mlp.up_proj.weight               -> blk.29.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.29.post_attention_layernorm.weight  -> blk.29.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.29.self_attn.k_proj.weight          -> blk.29.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.29.self_attn.o_proj.weight          -> blk.29.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.29.self_attn.q_proj.weight          -> blk.29.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.29.self_attn.v_proj.weight          -> blk.29.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.30.input_layernorm.weight           -> blk.30.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.30.mlp.down_proj.weight             -> blk.30.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.30.mlp.gate_proj.weight             -> blk.30.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.30.mlp.up_proj.weight               -> blk.30.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.30.post_attention_layernorm.weight  -> blk.30.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.30.self_attn.k_proj.weight          -> blk.30.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.30.self_attn.o_proj.weight          -> blk.30.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.30.self_attn.q_proj.weight          -> blk.30.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.30.self_attn.v_proj.weight          -> blk.30.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.31.input_layernorm.weight           -> blk.31.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.31.mlp.down_proj.weight             -> blk.31.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.31.mlp.gate_proj.weight             -> blk.31.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.31.mlp.up_proj.weight               -> blk.31.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.31.post_attention_layernorm.weight  -> blk.31.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.31.self_attn.k_proj.weight          -> blk.31.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.31.self_attn.o_proj.weight          -> blk.31.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.31.self_attn.q_proj.weight          -> blk.31.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.31.self_attn.v_proj.weight          -> blk.31.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.norm.weight                                -> output_norm.weight                       | BF16   | [4096]\n",
      "Writing ../results/ggufed/unsloth/mistral-7b-instruct-v0.2-bnb-4bit-unsloth.Q8_0.gguf, format 7\n",
      "Ignoring added_tokens.json since model matches vocab size without it.\n",
      "gguf: This GGUF file is for Little Endian only\n",
      "gguf: Setting special token type bos to 1\n",
      "gguf: Setting special token type eos to 2\n",
      "gguf: Setting special token type unk to 0\n",
      "gguf: Setting special token type pad to 0\n",
      "gguf: Setting add_bos_token to True\n",
      "gguf: Setting add_eos_token to False\n",
      "gguf: Setting chat_template to {{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\n",
      "[  1/291] Writing tensor token_embd.weight                      | size  32000 x   4096  | type Q8_0 | T+   8\n",
      "[  2/291] Writing tensor blk.0.attn_norm.weight                 | size   4096           | type F32  | T+   8\n",
      "[  3/291] Writing tensor blk.0.ffn_down.weight                  | size   4096 x  14336  | type Q8_0 | T+   8\n",
      "[  4/291] Writing tensor blk.0.ffn_gate.weight                  | size  14336 x   4096  | type Q8_0 | T+   8\n",
      "[  5/291] Writing tensor blk.0.ffn_up.weight                    | size  14336 x   4096  | type Q8_0 | T+   8\n",
      "[  6/291] Writing tensor blk.0.ffn_norm.weight                  | size   4096           | type F32  | T+   8\n",
      "[  7/291] Writing tensor blk.0.attn_k.weight                    | size   1024 x   4096  | type Q8_0 | T+   8\n",
      "[  8/291] Writing tensor blk.0.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+   8\n",
      "[  9/291] Writing tensor blk.0.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+   8\n",
      "[ 10/291] Writing tensor blk.0.attn_v.weight                    | size   1024 x   4096  | type Q8_0 | T+   8\n",
      "[ 11/291] Writing tensor blk.1.attn_norm.weight                 | size   4096           | type F32  | T+   8\n",
      "[ 12/291] Writing tensor blk.1.ffn_down.weight                  | size   4096 x  14336  | type Q8_0 | T+   8\n",
      "[ 13/291] Writing tensor blk.1.ffn_gate.weight                  | size  14336 x   4096  | type Q8_0 | T+   9\n",
      "[ 14/291] Writing tensor blk.1.ffn_up.weight                    | size  14336 x   4096  | type Q8_0 | T+  10\n",
      "[ 15/291] Writing tensor blk.1.ffn_norm.weight                  | size   4096           | type F32  | T+  10\n",
      "[ 16/291] Writing tensor blk.1.attn_k.weight                    | size   1024 x   4096  | type Q8_0 | T+  10\n",
      "[ 17/291] Writing tensor blk.1.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+  10\n",
      "[ 18/291] Writing tensor blk.1.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+  10\n",
      "[ 19/291] Writing tensor blk.1.attn_v.weight                    | size   1024 x   4096  | type Q8_0 | T+  10\n",
      "[ 20/291] Writing tensor blk.10.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  11\n",
      "[ 21/291] Writing tensor blk.10.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  11\n",
      "[ 22/291] Writing tensor blk.10.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  11\n",
      "[ 23/291] Writing tensor blk.10.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  11\n",
      "[ 24/291] Writing tensor blk.10.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  11\n",
      "[ 25/291] Writing tensor blk.10.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  11\n",
      "[ 26/291] Writing tensor blk.2.attn_norm.weight                 | size   4096           | type F32  | T+  11\n",
      "[ 27/291] Writing tensor blk.2.ffn_down.weight                  | size   4096 x  14336  | type Q8_0 | T+  12\n",
      "[ 28/291] Writing tensor blk.2.ffn_gate.weight                  | size  14336 x   4096  | type Q8_0 | T+  13\n",
      "[ 29/291] Writing tensor blk.2.ffn_up.weight                    | size  14336 x   4096  | type Q8_0 | T+  13\n",
      "[ 30/291] Writing tensor blk.2.ffn_norm.weight                  | size   4096           | type F32  | T+  14\n",
      "[ 31/291] Writing tensor blk.2.attn_k.weight                    | size   1024 x   4096  | type Q8_0 | T+  14\n",
      "[ 32/291] Writing tensor blk.2.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+  14\n",
      "[ 33/291] Writing tensor blk.2.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+  14\n",
      "[ 34/291] Writing tensor blk.2.attn_v.weight                    | size   1024 x   4096  | type Q8_0 | T+  14\n",
      "[ 35/291] Writing tensor blk.3.attn_norm.weight                 | size   4096           | type F32  | T+  14\n",
      "[ 36/291] Writing tensor blk.3.ffn_down.weight                  | size   4096 x  14336  | type Q8_0 | T+  14\n",
      "[ 37/291] Writing tensor blk.3.ffn_gate.weight                  | size  14336 x   4096  | type Q8_0 | T+  15\n",
      "[ 38/291] Writing tensor blk.3.ffn_up.weight                    | size  14336 x   4096  | type Q8_0 | T+  16\n",
      "[ 39/291] Writing tensor blk.3.ffn_norm.weight                  | size   4096           | type F32  | T+  16\n",
      "[ 40/291] Writing tensor blk.3.attn_k.weight                    | size   1024 x   4096  | type Q8_0 | T+  16\n",
      "[ 41/291] Writing tensor blk.3.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+  16\n",
      "[ 42/291] Writing tensor blk.3.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+  16\n",
      "[ 43/291] Writing tensor blk.3.attn_v.weight                    | size   1024 x   4096  | type Q8_0 | T+  16\n",
      "[ 44/291] Writing tensor blk.4.attn_norm.weight                 | size   4096           | type F32  | T+  16\n",
      "[ 45/291] Writing tensor blk.4.ffn_down.weight                  | size   4096 x  14336  | type Q8_0 | T+  17\n",
      "[ 46/291] Writing tensor blk.4.ffn_gate.weight                  | size  14336 x   4096  | type Q8_0 | T+  18\n",
      "[ 47/291] Writing tensor blk.4.ffn_up.weight                    | size  14336 x   4096  | type Q8_0 | T+  18\n",
      "[ 48/291] Writing tensor blk.4.ffn_norm.weight                  | size   4096           | type F32  | T+  18\n",
      "[ 49/291] Writing tensor blk.4.attn_k.weight                    | size   1024 x   4096  | type Q8_0 | T+  18\n",
      "[ 50/291] Writing tensor blk.4.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+  18\n",
      "[ 51/291] Writing tensor blk.4.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+  18\n",
      "[ 52/291] Writing tensor blk.4.attn_v.weight                    | size   1024 x   4096  | type Q8_0 | T+  18\n",
      "[ 53/291] Writing tensor blk.5.attn_norm.weight                 | size   4096           | type F32  | T+  18\n",
      "[ 54/291] Writing tensor blk.5.ffn_down.weight                  | size   4096 x  14336  | type Q8_0 | T+  19\n",
      "[ 55/291] Writing tensor blk.5.ffn_gate.weight                  | size  14336 x   4096  | type Q8_0 | T+  20\n",
      "[ 56/291] Writing tensor blk.5.ffn_up.weight                    | size  14336 x   4096  | type Q8_0 | T+  20\n",
      "[ 57/291] Writing tensor blk.5.ffn_norm.weight                  | size   4096           | type F32  | T+  20\n",
      "[ 58/291] Writing tensor blk.5.attn_k.weight                    | size   1024 x   4096  | type Q8_0 | T+  20\n",
      "[ 59/291] Writing tensor blk.5.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+  20\n",
      "[ 60/291] Writing tensor blk.5.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+  20\n",
      "[ 61/291] Writing tensor blk.5.attn_v.weight                    | size   1024 x   4096  | type Q8_0 | T+  20\n",
      "[ 62/291] Writing tensor blk.6.attn_norm.weight                 | size   4096           | type F32  | T+  20\n",
      "[ 63/291] Writing tensor blk.6.ffn_down.weight                  | size   4096 x  14336  | type Q8_0 | T+  21\n",
      "[ 64/291] Writing tensor blk.6.ffn_gate.weight                  | size  14336 x   4096  | type Q8_0 | T+  22\n",
      "[ 65/291] Writing tensor blk.6.ffn_up.weight                    | size  14336 x   4096  | type Q8_0 | T+  22\n",
      "[ 66/291] Writing tensor blk.6.ffn_norm.weight                  | size   4096           | type F32  | T+  22\n",
      "[ 67/291] Writing tensor blk.6.attn_k.weight                    | size   1024 x   4096  | type Q8_0 | T+  22\n",
      "[ 68/291] Writing tensor blk.6.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+  22\n",
      "[ 69/291] Writing tensor blk.6.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+  22\n",
      "[ 70/291] Writing tensor blk.6.attn_v.weight                    | size   1024 x   4096  | type Q8_0 | T+  22\n",
      "[ 71/291] Writing tensor blk.7.attn_norm.weight                 | size   4096           | type F32  | T+  22\n",
      "[ 72/291] Writing tensor blk.7.ffn_down.weight                  | size   4096 x  14336  | type Q8_0 | T+  23\n",
      "[ 73/291] Writing tensor blk.7.ffn_gate.weight                  | size  14336 x   4096  | type Q8_0 | T+  24\n",
      "[ 74/291] Writing tensor blk.7.ffn_up.weight                    | size  14336 x   4096  | type Q8_0 | T+  25\n",
      "[ 75/291] Writing tensor blk.7.ffn_norm.weight                  | size   4096           | type F32  | T+  25\n",
      "[ 76/291] Writing tensor blk.7.attn_k.weight                    | size   1024 x   4096  | type Q8_0 | T+  25\n",
      "[ 77/291] Writing tensor blk.7.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+  25\n",
      "[ 78/291] Writing tensor blk.7.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+  25\n",
      "[ 79/291] Writing tensor blk.7.attn_v.weight                    | size   1024 x   4096  | type Q8_0 | T+  25\n",
      "[ 80/291] Writing tensor blk.8.attn_norm.weight                 | size   4096           | type F32  | T+  25\n",
      "[ 81/291] Writing tensor blk.8.ffn_down.weight                  | size   4096 x  14336  | type Q8_0 | T+  25\n",
      "[ 82/291] Writing tensor blk.8.ffn_gate.weight                  | size  14336 x   4096  | type Q8_0 | T+  26\n",
      "[ 83/291] Writing tensor blk.8.ffn_up.weight                    | size  14336 x   4096  | type Q8_0 | T+  27\n",
      "[ 84/291] Writing tensor blk.8.ffn_norm.weight                  | size   4096           | type F32  | T+  27\n",
      "[ 85/291] Writing tensor blk.8.attn_k.weight                    | size   1024 x   4096  | type Q8_0 | T+  27\n",
      "[ 86/291] Writing tensor blk.8.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+  27\n",
      "[ 87/291] Writing tensor blk.8.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+  27\n",
      "[ 88/291] Writing tensor blk.8.attn_v.weight                    | size   1024 x   4096  | type Q8_0 | T+  27\n",
      "[ 89/291] Writing tensor blk.9.attn_norm.weight                 | size   4096           | type F32  | T+  27\n",
      "[ 90/291] Writing tensor blk.9.ffn_down.weight                  | size   4096 x  14336  | type Q8_0 | T+  28\n",
      "[ 91/291] Writing tensor blk.9.ffn_gate.weight                  | size  14336 x   4096  | type Q8_0 | T+  29\n",
      "[ 92/291] Writing tensor blk.9.ffn_up.weight                    | size  14336 x   4096  | type Q8_0 | T+  29\n",
      "[ 93/291] Writing tensor blk.9.ffn_norm.weight                  | size   4096           | type F32  | T+  29\n",
      "[ 94/291] Writing tensor blk.9.attn_k.weight                    | size   1024 x   4096  | type Q8_0 | T+  29\n",
      "[ 95/291] Writing tensor blk.9.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+  29\n",
      "[ 96/291] Writing tensor blk.9.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+  29\n",
      "[ 97/291] Writing tensor blk.9.attn_v.weight                    | size   1024 x   4096  | type Q8_0 | T+  29\n",
      "[ 98/291] Writing tensor blk.10.attn_norm.weight                | size   4096           | type F32  | T+  29\n",
      "[ 99/291] Writing tensor blk.10.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  30\n",
      "[100/291] Writing tensor blk.10.ffn_norm.weight                 | size   4096           | type F32  | T+  30\n",
      "[101/291] Writing tensor blk.11.attn_norm.weight                | size   4096           | type F32  | T+  30\n",
      "[102/291] Writing tensor blk.11.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  30\n",
      "[103/291] Writing tensor blk.11.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  31\n",
      "[104/291] Writing tensor blk.11.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  32\n",
      "[105/291] Writing tensor blk.11.ffn_norm.weight                 | size   4096           | type F32  | T+  32\n",
      "[106/291] Writing tensor blk.11.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  32\n",
      "[107/291] Writing tensor blk.11.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  32\n",
      "[108/291] Writing tensor blk.11.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  32\n",
      "[109/291] Writing tensor blk.11.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  32\n",
      "[110/291] Writing tensor blk.12.attn_norm.weight                | size   4096           | type F32  | T+  32\n",
      "[111/291] Writing tensor blk.12.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  32\n",
      "[112/291] Writing tensor blk.12.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  33\n",
      "[113/291] Writing tensor blk.12.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  34\n",
      "[114/291] Writing tensor blk.12.ffn_norm.weight                 | size   4096           | type F32  | T+  34\n",
      "[115/291] Writing tensor blk.12.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  34\n",
      "[116/291] Writing tensor blk.12.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  34\n",
      "[117/291] Writing tensor blk.12.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  34\n",
      "[118/291] Writing tensor blk.12.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  34\n",
      "[119/291] Writing tensor blk.13.attn_norm.weight                | size   4096           | type F32  | T+  34\n",
      "[120/291] Writing tensor blk.13.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  35\n",
      "[121/291] Writing tensor blk.13.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  37\n",
      "[122/291] Writing tensor blk.13.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  37\n",
      "[123/291] Writing tensor blk.13.ffn_norm.weight                 | size   4096           | type F32  | T+  37\n",
      "[124/291] Writing tensor blk.13.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  37\n",
      "[125/291] Writing tensor blk.13.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  37\n",
      "[126/291] Writing tensor blk.13.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  37\n",
      "[127/291] Writing tensor blk.13.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  37\n",
      "[128/291] Writing tensor blk.14.attn_norm.weight                | size   4096           | type F32  | T+  37\n",
      "[129/291] Writing tensor blk.14.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  37\n",
      "[130/291] Writing tensor blk.14.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  38\n",
      "[131/291] Writing tensor blk.14.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  38\n",
      "[132/291] Writing tensor blk.14.ffn_norm.weight                 | size   4096           | type F32  | T+  38\n",
      "[133/291] Writing tensor blk.14.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  38\n",
      "[134/291] Writing tensor blk.14.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  38\n",
      "[135/291] Writing tensor blk.14.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  38\n",
      "[136/291] Writing tensor blk.14.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  38\n",
      "[137/291] Writing tensor blk.15.attn_norm.weight                | size   4096           | type F32  | T+  38\n",
      "[138/291] Writing tensor blk.15.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  39\n",
      "[139/291] Writing tensor blk.15.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  40\n",
      "[140/291] Writing tensor blk.15.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  40\n",
      "[141/291] Writing tensor blk.15.ffn_norm.weight                 | size   4096           | type F32  | T+  41\n",
      "[142/291] Writing tensor blk.15.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  41\n",
      "[143/291] Writing tensor blk.15.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  41\n",
      "[144/291] Writing tensor blk.15.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  41\n",
      "[145/291] Writing tensor blk.15.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  41\n",
      "[146/291] Writing tensor blk.16.attn_norm.weight                | size   4096           | type F32  | T+  41\n",
      "[147/291] Writing tensor blk.16.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  41\n",
      "[148/291] Writing tensor blk.16.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  43\n",
      "[149/291] Writing tensor blk.16.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  43\n",
      "[150/291] Writing tensor blk.16.ffn_norm.weight                 | size   4096           | type F32  | T+  43\n",
      "[151/291] Writing tensor blk.16.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  43\n",
      "[152/291] Writing tensor blk.16.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  43\n",
      "[153/291] Writing tensor blk.16.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  43\n",
      "[154/291] Writing tensor blk.16.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  43\n",
      "[155/291] Writing tensor blk.17.attn_norm.weight                | size   4096           | type F32  | T+  43\n",
      "[156/291] Writing tensor blk.17.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  43\n",
      "[157/291] Writing tensor blk.17.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  44\n",
      "[158/291] Writing tensor blk.17.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  45\n",
      "[159/291] Writing tensor blk.17.ffn_norm.weight                 | size   4096           | type F32  | T+  45\n",
      "[160/291] Writing tensor blk.17.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  45\n",
      "[161/291] Writing tensor blk.17.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  45\n",
      "[162/291] Writing tensor blk.17.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  45\n",
      "[163/291] Writing tensor blk.17.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  45\n",
      "[164/291] Writing tensor blk.18.attn_norm.weight                | size   4096           | type F32  | T+  45\n",
      "[165/291] Writing tensor blk.18.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  46\n",
      "[166/291] Writing tensor blk.18.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  47\n",
      "[167/291] Writing tensor blk.18.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  47\n",
      "[168/291] Writing tensor blk.18.ffn_norm.weight                 | size   4096           | type F32  | T+  47\n",
      "[169/291] Writing tensor blk.18.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  47\n",
      "[170/291] Writing tensor blk.18.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  47\n",
      "[171/291] Writing tensor blk.18.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  47\n",
      "[172/291] Writing tensor blk.18.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  47\n",
      "[173/291] Writing tensor blk.19.attn_norm.weight                | size   4096           | type F32  | T+  47\n",
      "[174/291] Writing tensor blk.19.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  48\n",
      "[175/291] Writing tensor blk.19.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  49\n",
      "[176/291] Writing tensor blk.19.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  49\n",
      "[177/291] Writing tensor blk.19.ffn_norm.weight                 | size   4096           | type F32  | T+  50\n",
      "[178/291] Writing tensor blk.19.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  50\n",
      "[179/291] Writing tensor blk.19.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  50\n",
      "[180/291] Writing tensor blk.19.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  50\n",
      "[181/291] Writing tensor blk.19.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  50\n",
      "[182/291] Writing tensor blk.20.attn_norm.weight                | size   4096           | type F32  | T+  50\n",
      "[183/291] Writing tensor blk.20.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  50\n",
      "[184/291] Writing tensor blk.20.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  51\n",
      "[185/291] Writing tensor blk.20.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  52\n",
      "[186/291] Writing tensor blk.20.ffn_norm.weight                 | size   4096           | type F32  | T+  52\n",
      "[187/291] Writing tensor blk.20.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  52\n",
      "[188/291] Writing tensor blk.20.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  52\n",
      "[189/291] Writing tensor blk.20.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  52\n",
      "[190/291] Writing tensor blk.20.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  52\n",
      "[191/291] Writing tensor blk.21.attn_norm.weight                | size   4096           | type F32  | T+  52\n",
      "[192/291] Writing tensor blk.21.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  52\n",
      "[193/291] Writing tensor blk.21.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  54\n",
      "[194/291] Writing tensor blk.21.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  54\n",
      "[195/291] Writing tensor blk.21.ffn_norm.weight                 | size   4096           | type F32  | T+  54\n",
      "[196/291] Writing tensor blk.21.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  54\n",
      "[197/291] Writing tensor blk.21.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  54\n",
      "[198/291] Writing tensor blk.21.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  54\n",
      "[199/291] Writing tensor blk.21.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  54\n",
      "[200/291] Writing tensor blk.22.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  54\n",
      "[201/291] Writing tensor blk.22.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  54\n",
      "[202/291] Writing tensor blk.22.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  54\n",
      "[203/291] Writing tensor blk.22.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  54\n",
      "[204/291] Writing tensor output.weight                          | size  32000 x   4096  | type Q8_0 | T+  57\n",
      "[205/291] Writing tensor blk.22.attn_norm.weight                | size   4096           | type F32  | T+  57\n",
      "[206/291] Writing tensor blk.22.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  57\n",
      "[207/291] Writing tensor blk.22.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  57\n",
      "[208/291] Writing tensor blk.22.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  57\n",
      "[209/291] Writing tensor blk.22.ffn_norm.weight                 | size   4096           | type F32  | T+  57\n",
      "[210/291] Writing tensor blk.23.attn_norm.weight                | size   4096           | type F32  | T+  57\n",
      "[211/291] Writing tensor blk.23.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  58\n",
      "[212/291] Writing tensor blk.23.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  59\n",
      "[213/291] Writing tensor blk.23.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  59\n",
      "[214/291] Writing tensor blk.23.ffn_norm.weight                 | size   4096           | type F32  | T+  59\n",
      "[215/291] Writing tensor blk.23.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  59\n",
      "[216/291] Writing tensor blk.23.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  59\n",
      "[217/291] Writing tensor blk.23.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  59\n",
      "[218/291] Writing tensor blk.23.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  59\n",
      "[219/291] Writing tensor blk.24.attn_norm.weight                | size   4096           | type F32  | T+  59\n",
      "[220/291] Writing tensor blk.24.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  60\n",
      "[221/291] Writing tensor blk.24.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  61\n",
      "[222/291] Writing tensor blk.24.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  61\n",
      "[223/291] Writing tensor blk.24.ffn_norm.weight                 | size   4096           | type F32  | T+  61\n",
      "[224/291] Writing tensor blk.24.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  61\n",
      "[225/291] Writing tensor blk.24.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  61\n",
      "[226/291] Writing tensor blk.24.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  61\n",
      "[227/291] Writing tensor blk.24.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  61\n",
      "[228/291] Writing tensor blk.25.attn_norm.weight                | size   4096           | type F32  | T+  61\n",
      "[229/291] Writing tensor blk.25.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  62\n",
      "[230/291] Writing tensor blk.25.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  63\n",
      "[231/291] Writing tensor blk.25.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  63\n",
      "[232/291] Writing tensor blk.25.ffn_norm.weight                 | size   4096           | type F32  | T+  63\n",
      "[233/291] Writing tensor blk.25.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  63\n",
      "[234/291] Writing tensor blk.25.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  63\n",
      "[235/291] Writing tensor blk.25.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  63\n",
      "[236/291] Writing tensor blk.25.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  63\n",
      "[237/291] Writing tensor blk.26.attn_norm.weight                | size   4096           | type F32  | T+  63\n",
      "[238/291] Writing tensor blk.26.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  64\n",
      "[239/291] Writing tensor blk.26.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  65\n",
      "[240/291] Writing tensor blk.26.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  65\n",
      "[241/291] Writing tensor blk.26.ffn_norm.weight                 | size   4096           | type F32  | T+  65\n",
      "[242/291] Writing tensor blk.26.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  65\n",
      "[243/291] Writing tensor blk.26.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  65\n",
      "[244/291] Writing tensor blk.26.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  65\n",
      "[245/291] Writing tensor blk.26.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  65\n",
      "[246/291] Writing tensor blk.27.attn_norm.weight                | size   4096           | type F32  | T+  65\n",
      "[247/291] Writing tensor blk.27.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  66\n",
      "[248/291] Writing tensor blk.27.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  67\n",
      "[249/291] Writing tensor blk.27.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  67\n",
      "[250/291] Writing tensor blk.27.ffn_norm.weight                 | size   4096           | type F32  | T+  67\n",
      "[251/291] Writing tensor blk.27.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  67\n",
      "[252/291] Writing tensor blk.27.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  67\n",
      "[253/291] Writing tensor blk.27.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  67\n",
      "[254/291] Writing tensor blk.27.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  67\n",
      "[255/291] Writing tensor blk.28.attn_norm.weight                | size   4096           | type F32  | T+  67\n",
      "[256/291] Writing tensor blk.28.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  68\n",
      "[257/291] Writing tensor blk.28.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  69\n",
      "[258/291] Writing tensor blk.28.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  69\n",
      "[259/291] Writing tensor blk.28.ffn_norm.weight                 | size   4096           | type F32  | T+  69\n",
      "[260/291] Writing tensor blk.28.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  69\n",
      "[261/291] Writing tensor blk.28.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  69\n",
      "[262/291] Writing tensor blk.28.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  69\n",
      "[263/291] Writing tensor blk.28.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  69\n",
      "[264/291] Writing tensor blk.29.attn_norm.weight                | size   4096           | type F32  | T+  69\n",
      "[265/291] Writing tensor blk.29.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  70\n",
      "[266/291] Writing tensor blk.29.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  71\n",
      "[267/291] Writing tensor blk.29.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  71\n",
      "[268/291] Writing tensor blk.29.ffn_norm.weight                 | size   4096           | type F32  | T+  71\n",
      "[269/291] Writing tensor blk.29.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  71\n",
      "[270/291] Writing tensor blk.29.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  71\n",
      "[271/291] Writing tensor blk.29.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  72\n",
      "[272/291] Writing tensor blk.29.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  72\n",
      "[273/291] Writing tensor blk.30.attn_norm.weight                | size   4096           | type F32  | T+  72\n",
      "[274/291] Writing tensor blk.30.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  72\n",
      "[275/291] Writing tensor blk.30.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  73\n",
      "[276/291] Writing tensor blk.30.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  73\n",
      "[277/291] Writing tensor blk.30.ffn_norm.weight                 | size   4096           | type F32  | T+  74\n",
      "[278/291] Writing tensor blk.30.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  74\n",
      "[279/291] Writing tensor blk.30.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  74\n",
      "[280/291] Writing tensor blk.30.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  74\n",
      "[281/291] Writing tensor blk.30.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  74\n",
      "[282/291] Writing tensor blk.31.attn_norm.weight                | size   4096           | type F32  | T+  74\n",
      "[283/291] Writing tensor blk.31.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  74\n",
      "[284/291] Writing tensor blk.31.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  75\n",
      "[285/291] Writing tensor blk.31.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  75\n",
      "[286/291] Writing tensor blk.31.ffn_norm.weight                 | size   4096           | type F32  | T+  75\n",
      "[287/291] Writing tensor blk.31.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  75\n",
      "[288/291] Writing tensor blk.31.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  75\n",
      "[289/291] Writing tensor blk.31.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  75\n",
      "[290/291] Writing tensor blk.31.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  75\n",
      "[291/291] Writing tensor output_norm.weight                     | size   4096           | type F32  | T+  75\n",
      "Wrote ../results/ggufed/unsloth/mistral-7b-instruct-v0.2-bnb-4bit-unsloth.Q8_0.gguf\n",
      "Unsloth: Conversion completed! Output location: ./../results/ggufed/unsloth/mistral-7b-instruct-v0.2-bnb-4bit-unsloth.Q8_0.gguf\n"
     ]
    }
   ],
   "source": [
    "main(\n",
    "    model_name=\"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n",
    "    dataset_size=dataset_size,\n",
    "    batch_size=64,\n",
    "    max_seq_length=max_seq_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5426b11-14ae-47ed-bf62-33dc71084072",
   "metadata": {},
   "source": [
    "## Trained model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a4a39ed-a866-40df-b497-7e597de6468c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensure_reproducibility();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95a91533-eb4c-463d-9763-5d0069418d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Mistral patching release 2024.1\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3090. Max memory: 23.669 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.1.2. CUDA = 8.6. CUDA Toolkit = 11.8.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.23.post1. FA = True.\n",
      " \"-____-\"     Apache 2 free license: http://github.com/unslothai/unsloth\n",
      "You passed `quantization_config` to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` attribute will be overwritten with the one you passed to `from_pretrained`.\n",
      "Unsloth 2024.1 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_model(\n",
    "    model_name=\"../results/unsloth/mistral-7b-instruct-v0.2-bnb-4bit/\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    with_lora=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9564a3d2-c857-46dc-8ea1-4f00ef256ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-28 20:06:06 - hate-detection.data:46 - INFO - Loaded dataset ucberkeley-dlab/measuring-hate-speech with 11520 training examples and 2880 test examples\n"
     ]
    }
   ],
   "source": [
    "train_dataset, test_dataset = load_dataset(size=dataset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8da0ec7-1590-4801-a307-ab209799154a",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = open(os.path.join(\"config\", \"TEMPLATE\")).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c83385f0-b94b-4a95-91c9-6d2e532bf981",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_fn = partial(generate, model=model, tokenizer=tokenizer, template=template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71f35976-efc6-42d6-9090-727b6ea7be34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'Human trafficking is a form of modern slavery and vulnerable people, especially women and girls, get trapped and exploited every day. Alberta will be taking real action to fight it and support victims and those at risk. URL #ableg #abpoli',\n",
       " 'completion': 'Supportive'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18c71f14-846e-4a11-a009-f1af3a578fa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Supportive']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_fn(test_dataset[0]['prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73e0e61f-714e-4323-a597-c390c98ce901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': '>Some incels give a lot but get nothing back   And so should be rewarded with pussy? Are you aware that said pussy is attached to a fully realized human being with the exact same natural rights to happiness as you? And that she does not deserve to be forced to have sex with anyone she finds undesirable?    Do you get that women have the same right to reject men that men have to reject women?    How committed are you to the belief that women are holes, rather than human beings deserving of respect?',\n",
       " 'completion': 'Supportive'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e137ebea-e218-4fba-986c-543bdd186984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Supportive']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_fn(test_dataset[-1]['prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d93f2e38-4684-497c-8c83-b14495bd8559",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|███████████████████████████████████████████████████████████████████████| 90/90 [02:45<00:00,  1.84s/it]\n"
     ]
    }
   ],
   "source": [
    "eval_results = evaluate_model(generate_fn=generate_fn, test_dataset=test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83ac7aba-4d7b-459f-90c7-ad5870ed08c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Hate       0.75      0.91      0.82      1049\n",
      "     Neutral       0.69      0.29      0.41       712\n",
      "  Supportive       0.78      0.91      0.84      1119\n",
      "     Unknown       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.76      2880\n",
      "   macro avg       0.56      0.53      0.52      2880\n",
      "weighted avg       0.75      0.76      0.73      2880\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(eval_results['classification_report'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df12f629-53af-4873-b537-4899e4ffd7e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ConfusionMatrixDisplay(eval_results['confusion_matrix'], display_labels=[\"Hate\", \"Neutral\", \"Supportive\", \"Unknown\"]).plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a86a0e-08ae-490e-893c-88a76f2a89d8",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a81bd2e-8df8-4095-b7ef-bc315659579a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3a8d90cd-8ea5-4026-9fc9-d099fd1f6471",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"http://127.0.0.1:5000/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d0da5c04-d849-40de-8091-5fa00a2a7ff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.get(base_url + 'healthcheck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7c89da6c-4b6b-4bd5-b0c6-688d68fddb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "hate_tweet = \"This is what low IQ looks like.  Australia spends $8.5 billion on beef per year. Does that grow on trees? You fucking ape.  Also, we slaughter kheer/firni on Eid ul Fitr not animals. URL\"\n",
    "sup_tweet = \"Human trafficking is a form of modern slavery and vulnerable people, especially women and girls, get trapped and exploited every day. Alberta will be taking real action to fight it and support victims and those at risk. URL #ableg #abpoli'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9fb86e85-9b55-4206-aa6b-f644b7081dbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Neutral'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.post(base_url + \"infer\", data=hate_tweet).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f0e3526d-f366-4ea6-a70d-faca2628e762",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Supportive'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.post(base_url + \"infer\", data=sup_tweet).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6914598a-26e6-4a80-ad90-b68fce92719e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:hate-detection]",
   "language": "python",
   "name": "conda-env-hate-detection-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
